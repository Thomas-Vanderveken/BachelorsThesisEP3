\chapter{\IfLanguageName{dutch}{Stand van zaken}{State of the art}}%
\label{ch:stand-van-zaken}

% Tip: Begin elk hoofdstuk met een paragraaf inleiding die beschrijft hoe
% dit hoofdstuk past binnen het geheel van de bachelorproef. Geef in het
% bijzonder aan wat de link is met het vorige en volgende hoofdstuk.

% Pas na deze inleidende paragraaf komt de eerste sectiehoofding.

De Securities and Exchange Commission (SEC) vereist dat institutionele vermogensbeheerders een kwartaalrapport indienen dat bekend staat als Form 13F als ze zeggenschap hebben over \$100 miljoen of meer in Section 13(f) effecten. Sectie 13(f) van de Securities Exchange Act van 1934 verplicht de openbaarmaking van effectenbezit door grote institutionele beleggers om de transparantie te vergroten. In 1975 implementeerde het Congres deze bepaling om de toegankelijkheid van informatie over de investeringsactiviteiten van deze bedrijven te verbeteren. De bedoeling was om het vertrouwen van beleggers in de integriteit van de effectenmarkten in de Verenigde Staten te vergroten door middel van een openbaarmakingsprogramma\autocite{SECform13F2024}.\\
Formulier 13F biedt een uitgebreid overzicht van de aandelenbeleggingen van prominente beleggingsmaatschappijen wereldwijd en is een zeer belangrijk hulpmiddel voor analisten, onderzoekers en beleggers die inzicht willen krijgen in markttrends en de beleggingsbenaderingen van belangrijke marktspelers. Het onverwerkte tekstformaat waarin deze inzendingen worden aangeleverd, vormt echter een aanzienlijke belemmering voor effectieve gegevensextractie en -analyse, vooral voor inzendingen van vóór 2013. Vóór 2013 ontbrak het bij 13F-aanmeldingen vaak aan standaardisatie en systematische opmaak, wat nu wel gebruikelijk is bij recentere aanmeldingen.\\
Kunstmatige intelligentie (AI) en machine learning (ML) technologieën hebben de extractie en organisatie van gegevens uit ongestructureerde tekst de afgelopen jaren aanzienlijk veranderd. Geavanceerde methodologieën zoals Natural Language Processing (NLP) en deep learning modellen vergemakkelijken de omzetting van tekstuele 13F aanvragen in gestructureerde datasets die geschikt zijn voor grondige analyse en studie. Standaardisatie is cruciaal voor historische gegevens, omdat het ontbreken van uniformiteit geautomatiseerde verwerking kan bemoeilijken. Door gebruik te maken van deze technologieën kunnen we zowel huidige als vroegere 13F aanvragen omzetten in georganiseerde gegevens, die vervolgens kunnen worden opgeslagen in databases, waardoor patronen eenvoudiger kunnen worden opgehaald, gevisualiseerd en geanalyseerd.\\
\\
Het doel van deze literatuurstudie is het onderzoeken en beoordelen van de verschillende Artificial Intelligence (AI) en Machine Learning (ML) technieken die kunnen worden gebruikt om gegevens uit 13F-formulieren van voor 2013 te extraheren, te organiseren en op te slaan. Het doel van het onderzoek is het bepalen van de meest efficiënte methoden om de ongeorganiseerde inhoud van deze documenten om te zetten in een gestructureerd formaat dat geschikt is voor analyse en opslag in een database. Dit houdt in dat er een vergelijkend onderzoek wordt gedaan naar verschillende kunstmatige intelligentie methodologieën, zoals Natural Language Processing (NLP) en Deep Learning modellen, en dat bepaalde tools zoals BERT, GPT en SpaCy worden geëvalueerd. De evaluatie zal ook de integratie van gestructureerde gegevens in databasemanagementsystemen (DBMS) onderzoeken, om te garanderen dat de geëxtraheerde gegevens gemakkelijk beschikbaar zijn voor later onderzoek en analyse. Het doel van deze evaluatie is om een uitgebreide kennis te krijgen van de meest effectieve procedures en technologie voor het verwerken van 13F-formulieren. 

% ----------------------------------------------------------------------------
\section{Wat zijn 13F meldingen}


Hier bespreken we kort wat 13F-meldingen zijn, we gaan dieper in op waarvoor ze dienen, welke structuur ze hebben, en wat er in vermeld word.
\subsection{Definitie en doel}
13F-aanmeldingen zijn verplichte wettelijke documenten die de Amerikaanse Securities and Exchange Commission (SEC) vereist onder Sectie 13(f) van de Securities Exchange Act van 1934. Deze deponeringen worden gebruikt om de portefeuilles van institutionele beleggingsbeheerders te rapporteren.

Doel: Het belangrijkste doel van 13F filings is om duidelijkheid en openheid te bieden over de beleggingsactiviteiten van belangrijke institutionele beleggers. Deze vereiste vergemakkelijkt het toezicht op beleggingsposities van verschillende instellingen, zoals beleggingsfondsen, pensioenfondsen en andere belangrijke beleggingsbeheerders, door het publiek en regelgevende instanties.


\subsection{Belanrijke kenmerken}

\subsubsection{Vereisten voor rapportage:}

Frequentie: Institutionele beleggingsbeheerders met minimaal \$100 miljoen aan beheerd vermogen moeten elk kwartaal een 13F-rapport indienen.
Inhoud: De rapporten bevatten uitgebreide informatie over het aandelenbezit van de instelling, waaronder de naam, het tickersymbool, het aantal aandelen en de marktwaarde.
\subsubsection{Omvang van de informatie:}

Aandelenbezit: De openbaarmakingen concentreren zich voornamelijk op aandelen, terwijl andere soorten activa zoals obligaties, derivaten en private equity worden uitgesloten.
Disclaimer: Elk bestand biedt een kort overzicht van de aandelenportefeuille van de instelling aan het einde van de rapportageperiode, wat een waardevol inzicht geeft in hun investeringsmethoden.
\subsubsection{Opmaak en toegankelijkheid:}

Vanaf 2013 is het verplicht om alle 13F-dossiers elektronisch in te dienen via het EDGAR-systeem van de SEC. De elektronische deponeringen zijn gemakkelijk toegankelijk voor het publiek, wat transparantie garandeert en het bestuderen van het materiaal vergemakkelijkt.
Papieren indieningen (vóór 2013): Vóór 2013 werd een aanzienlijk aantal 13F-dossiers op papier ingediend, wat resulteerde in een moeilijker en tijdrovender proces om toegang te krijgen tot de informatie en deze te analyseren. Deze dossiers moesten vaak door mensen in systemen worden ingevoerd voor verdere verwerking.

Illustraties van 13F indieningsformaten:

\begin{itemize}
  \item Voorbeeld van papieren indiening vóór 2013:
  \begin{itemize}
    \item Dit is een gedigitaliseerde afbeelding van een standaard 13F-bestand dat traditioneel op papier werd ingediend voordat elektronische indiening verplicht werd. Dit formaat omvatte handmatig geschreven of getypte informatie over de activa van de instelling.
    \item Het analyseren van papieren bestanden leverde aanzienlijke problemen op bij het extraheren en analyseren van gegevens, waardoor het gebruik van handmatige gegevensinvoer- en validatieprocedures noodzakelijk werd.
  \end{itemize}
  \item Na het jaar 2013, als voorbeeld van elektronische archivering:

  \begin{itemize}
    \item Vanaf 2013 werd het elektronische formaat gestandaardiseerd, waardoor onmiddellijke toegang tot en analyse van deponeringen vanuit het EDGAR-systeem mogelijk werd. Hieronder ziet u een momentopname van een elektronische 13F indiening.
    \item Het gebruik van elektronische deponeringen heeft de efficiëntie van gegevensanalyse aanzienlijk verbeterd door geautomatiseerde extractie en vereenvoudigde aggregatie van financiële gegevens mogelijk te maken.
  \end{itemize}

\end{itemize}



Historisch onderzoek van deponeringen van vóór 2013: 
\begin{itemize}
  \item Hoewel deponeringen van na 2013 gemakkelijk beschikbaar zijn en door machines verwerkt kunnen worden, bieden deponeringen van voor 2013 rijke historische informatie die essentieel is voor het doen van diepgaande marktanalyses en onderzoek over een langere periode. Niettemin moeten de eerdere dossiers meer bewerkingen ondergaan om de gegevens te converteren en te organiseren voor analyse.
  \item Er ontstaan uitdagingen bij het extraheren van gegevens wanneer geprobeerd wordt om informatie uit papieren bestanden van voor 2013 te halen. Dit proces vereist het gebruik van OCR-technologie (Optical Character Recognition), handmatige gegevensinvoer of methoden voor gegevensinvoer via crowdsourcing. Deze gebreken en inconsistenties moeten in elke analyse worden meegenomen.

  \item De periode voorafgaand aan het elektronische indieningsmandaat in 2013 markeerde een belangrijke verschuiving in de rapportage en toegankelijkheid van financiële gegevens. Deze periode toonde de vooruitgang in financiële rapportagepraktijken en het toenemende belang van digitale gegevensverwerking in financiële analyses.

\end{itemize}

Samenvatting van de structuur en het gebruiksgemak
De overgang van papieren naar elektronische 13F-bestanden betekende een aanzienlijke verbetering in de beschikbaarheid en gebruiksvriendelijkheid van financiële gegevens voor zowel beleggers als wetenschappers. Desondanks heeft het bestaan van bestanden die vóór 2013 zijn gemaakt specifieke moeilijkheden en voordelen voor het uitvoeren van historische analyses. Het is van cruciaal belang om de verschillende formaten en hun gevolgen voor de extractie en analyse van gegevens te begrijpen.


% ----------------------------------------------------------------------------




\section{AI en Machine learning in financiële data extractie}
Dit hoofdstuk onderzoekt de significante invloed van kunstmatige intelligentie (AI) en machinaal leren (ML) op het ophalen van financiële gegevens, met name op 13F-formulieren van vóór 2013. Deze tekst presenteert de basisprincipes van kunstmatige intelligentie (AI) en machinaal leren (ML), onderzoekt hoe ze worden gebruikt bij het extraheren van financiële gegevens en analyseert praktijkvoorbeelden die de doeltreffendheid ervan aantonen. Het hoofdstuk bespreekt ook specifieke moeilijkheden in verband met gegevenskwaliteit, de veranderende financiële terminologie en de noodzaak om AI-modellen aan te passen en aan te passen om de complexiteit van historische financiële documentatie effectief te beheren.
2.1 Introduction to AI and Machine Learning
2.1.1 Overview of AI and Machine Learning
Artificial Intelligence (AI) and Machine Learning (ML) have revolutionized the way data is processed and analyzed, particularly in fields that deal with large volumes of unstructured or semi-structured data, such as finance. AI involves the development of systems capable of performing tasks that typically require human intelligence, while ML, a subset of AI, focuses on the creation of algorithms that can learn from data and make predictions or decisions.

2.1.2 Applications of AI in Financial Data Extraction
In the financial sector, AI is used extensively for tasks like automated trading, risk assessment, and regulatory compliance. Specifically, AI plays a crucial role in the extraction of data from financial documents. Techniques such as Natural Language Processing (NLP) and Optical Character Recognition (OCR) are essential for interpreting and converting textual information from forms like 13F filings into structured data that can be analyzed.

2.1.3 Case Studies in AI-Driven Data Extraction
There are numerous examples where AI has been applied successfully to extract data from financial documents. For instance, AI-driven systems have been developed to analyze 10-K and 10-Q filings, which share similarities with 13F forms in terms of structure and complexity. These case studies highlight the potential for AI to automate and improve the accuracy of data extraction processes, particularly for regulatory filings.

2.2 Challenges in Applying AI to Pre-2013 13F Forms
2.2.1 Data Quality and Format Issues
Pre-2013 13F forms present unique challenges due to their non-standardized formats and the potential for poor data quality, such as low-resolution scans or handwritten annotations. These factors can significantly hinder the effectiveness of AI tools, particularly OCR systems, which rely on clear and consistent text patterns.

2.2.2 Evolution of Financial Language
The financial terminology used in older 13F forms may differ from modern usage, posing additional challenges for NLP models that are trained on contemporary datasets. This evolution of language requires the customization of AI tools to accurately interpret and extract relevant information from these historical documents.

2.2.3 Customization and Adaptation of AI Models
Addressing these challenges requires adapting existing AI models. For example, OCR models might need retraining to recognize outdated fonts or handwritten text, while NLP models may require fine-tuning to understand the specific financial terminology of the time. Additionally, hybrid approaches that combine rule-based systems with machine learning may be necessary to handle the inconsistencies and complexities of these older documents.
% ----------------------------------------------------------------------------

\section{Text mining en gerelateerde technieken}
Text mining, of ook bekend tekstdatamining, is de procedure om ongestructureerde tekst om te zetten in een gestructureerd formaat om significante patronen te ontdekken en nieuwe inzichten te verwerven. Text mining maakt de analyse van uitgebreide tekstdatasets mogelijk om significante thema's, patronen en verborgen verbanden bloot te leggen. Deze techniek is essentieel voor het omzetten van ongestructureerde gegevens in gestructureerde gegevens, die vervolgens kunnen worden gebruikt voor analyse en besluitvorming\autocite{IBM2024}.

\subsection{Document datatypes}
Text mining kan verschillende soorten gegevens omvatten, waaronder:

\begin{itemize}
  \item \textbf{Gestructureerde Gegevens}: Deze gegevens zijn gestandaardiseerd in een tabelvorm, wat ze makkelijker maakt om op te slaan en te verwerken voor analyse en machine learning-algoritmen. Voorbeelden includeren databanken met kolommen en rijen%TODO INSERT IMAGE
  \item \textbf{Ongestructureerde Gegevens}: Deze gegevens hebben geen vooraf gedefinieerd formaat en kunnen tekst uit bronnen zoals sociale media of productrecensies bevatten, evenals rijke media zoals video- en audiobestanden. Aangezien financiële documenten vaak in ongestructureerd formaat bestaan, is text mining essentieel om deze gegevens om te zetten in een bruikbaar formaat.
  \item \textbf{Semi-gestructureerde Gegevens}: Deze gegevens vormen een mix tussen gestructureerde en ongestructureerde formaten. Ze hebben enige organisatie, maar voldoen niet volledig aan de vereisten van een relationele database. Voorbeelden hiervan zijn XML, JSON en HTML-bestanden.
\end{itemize}

Dit onderscheid zijn van groot belang voor het begrijpen van hoe text mining toegepast kan worden over de verschillende data structuren, dit opent de mogelijkheid om de data de extraheren en belanrijke inzichten te verwerven\autocite{AWS2024}.

\subsection{Text mining vs. Text analytics}
Hoewel text mining en text analytics vaak door elkaar worden gebruikt, kan er een genuanceerd onderscheid tussen de twee bestaan. Bij text mining gaat het meestal om het identificeren van patronen en trends in ongestructureerde gegevens, terwijl text analytics gericht is op het afleiden van kwantitatieve inzichten door gegevens op een gestructureerde manier te analyseren. Deze observaties kunnen vervolgens grafisch worden weergegeven om de ontdekkingen effectief over te brengen aan een breder publiek.\autocite{IBM2024}
\subsubsection{Text mining: vinden van verstopte patronen}
Text mining omvat het extraheren van waardevolle informatie en het identificeren van verborgen patronen uit uitgebreide verzamelingen ongeorganiseerde of gedeeltelijk georganiseerde tekstuele gegevens. Text mining is een gespecialiseerde vorm van datamining die is ontworpen om vooral tekstuele informatie te verwerken. Het belangrijkste doel van text mining is om tekst om te zetten in analyseerbare gegevens om inzichten, trends en patronen te ontdekken die niet direct voor de hand liggen. Deze aanpak omvat een reeks methodologieën, waaronder het ophalen van informatie, natuurlijke taalverwerking (NLP) en machinaal leren. Het primaire doel is het begrijpen en analyseren van uitgebreide tekstdatabases \autocite{gaikwad2014text}.

\paragraph{Voor en nadelen text mining}
Volgens \autocite{Kinter2024} en \autocite{gaikwad2014text} zijn er veel voor en nadelen aan text mining:
\begin{itemize}
    \item Voordelen:
  \begin{itemize}
    \item Het corpus van teksten kan worden geanalyseerd met technieken zoals informatie-extractie om de namen van verschillende entiteiten en hun relaties te identificeren. 
    \item De complexe taak om effectief om te gaan met grote hoeveelheden ongestructureerde gegevens om patronen bloot te leggen, wordt aangepakt door het gebruik van text mining.
    \item Bedrijven kunnen een uitgebreid inzicht krijgen in huidige trends en patronen door inzichten te analyseren die zijn verkregen uit vele gegevensbronnen. Deze inzichten helpen bedrijven bij het nemen van weloverwogen zakelijke beslissingen.

  \end{itemize}
  \item Nadelen
  \begin{itemize}
    \item Text mining gebruikt vaak een grote hoeveelheid gegevens. Het efficiënt opslaan, beheren en verwerken van deze gegevens vereist daarom een grote hoeveelheid opslagruimte en rekenkracht, wat duur kan zijn.
    \item Text mining, gegevensanalyse en patroonherkenning zijn sterk afhankelijk van de kwaliteit van de gegevens. De nauwkeurigheid van de resultaten kan worden beïnvloed door variaties in de gegevenskwaliteit, die worden beïnvloed door de structuur en voorbewerking van de gegevens.
  \end{itemize}
\end{itemize}

\subsubsection{Text analyse: het afleiden van semantische beteknis}
Tekstanalyse daarentegen houdt zich meer bezig met het begrijpen en interpreteren van de inhoud van tekst om er informatie van hoge kwaliteit uit af te leiden. In tegenstelling tot text mining, dat zich richt op het ontdekken van nieuwe patronen, is tekstanalyse gericht op het extraheren en interpreteren van bestaande informatie uit tekstgegevens. Dit proces omvat de toepassing van semantische analysetechnieken om de betekenis, context en bedoeling achter de woorden in de tekst te begrijpen (International Journal of Computer Applications, 2014).

Tekstanalyse maakt vaak gebruik van Natural Language Processing (NLP) om de structuur van zinnen te ontleden, entiteiten te identificeren en sentiment te analyseren. Deze technieken zijn cruciaal voor taken zoals sentimentanalyse, waarbij het doel is om de emotionele toon van een tekst te bepalen, of onderwerpmodellering, waarbij het doel is om de belangrijkste thema's te identificeren die in een set documenten worden besproken. Tekstanalyse kan ook meer geavanceerde methoden omvatten, zoals entiteitherkenning, waarbij belangrijke stukken informatie (zoals namen, data en locaties) in een tekst worden geïdentificeerd en geclassificeerd.

\paragraph{conclusie}
Terwijl text mining vaak verkennend is, waarbij gezocht wordt naar onbekende patronen, is tekstanalyse gerichter, waarbij de nadruk ligt op het extraheren van specifieke informatie van hoge kwaliteit uit de tekst. In een juridische context kan tekstanalyse bijvoorbeeld worden gebruikt om relevante clausules uit een contract te halen, terwijl text mining kan worden gebruikt om trends in juridische beslissingen in de loop van de tijd te identificeren (International Journal of Computer Applications, 2014).


\subsection{Text mining techniques}
\autocite{Talib2016TextMining} spreekt over enkele technieken zoals Information Extraction (IE), Information retrieval (IR), en Meerdere NLP technieken die gebruikt worden in data mining, deze zullen hier besproken worden

\subsubsection{Information retreival vs Information extraction}
\paragraph{Information retrieval}
Informatieophaling (Information Retrieval, IR) verwijst naar de interactie tussen mens en computer wanneer een gebruiker informatie zoekt die overeenkomt met zijn of haar zoekopdracht in een database of computersysteem. Dit proces omvat het ophalen van relevante inhoud op basis van de behoeften van de gebruiker. Het systeem vergelijkt de zoekopdracht van de gebruiker met een reeks documenten om de meest relevante te identificeren en presenteert deze uiteindelijk in een geprioriteerde lijst. Dit gespecialiseerde vakgebied, zoals beschreven door Krallinger (2024), is essentieel om gebruikers in staat te stellen snel en efficiënt informatie te lokaliseren en extraheren uit uitgebreide en vaak ongestructureerde gegevensbronnen zoals tekstdocumenten, databases of het internet.

De effectiviteit van een IR-systeem wordt gemeten aan de hand van metrics zoals precisie en recall. Precisie is de verhouding tussen het aantal relevante documenten dat wordt opgehaald en het totale aantal opgehaalde documenten, terwijl recall de verhouding is tussen het aantal relevante documenten dat wordt opgehaald en het totale aantal relevante documenten in de dataset \autocite{Javija2024}. Deze metrics helpen om informatie-overload te verminderen door ervoor te zorgen dat alleen de meest relevante informatie aan de gebruiker wordt gepresenteerd. De methoden en technieken die worden gebruikt in IR-systemen zijn fundamenteel voor het aandrijven van technologieën zoals zoekmachines, die een snelle en efficiënte informatieophaling mogelijk maken \autocite{Krallinger2024}.

Enkele IR technieken zijn maar niet gelimiteerd tot \autocite{IBM2024}:
\begin{itemize}
  \item Tokenizatie: Dit is het proces van het opbreken van text in zinnen en woorden genoemd 'tokens'. Deze zijn dan gebruikt in de modellen voor clustering en documentmatching taken\autocite{IBM2024}.
  \item Stemming is een tekstvoorbewerkingsmethode die wordt gebruikt in natuurlijke taalverwerking (NLP) om woorden te vereenvoudigen door ze om te zetten naar hun basisvorm. Het doel van stemming is om woorden te stroomlijnen en te normaliseren en zo de efficiëntie van het ophalen van informatie, het categoriseren van teksten en andere natuurlijke taalverwerkingsactiviteiten (NLP) te verbeteren\autocite{SC2024}.
\end{itemize}

\paragraph{Information extraction}
Information Extraction (IE) aims to extract structured information from unstructured documents using techniques like Natural Language Processing (NLP). Unlike Information Retrieval (IR), which retrieves relevant documents, IE focuses on identifying specific data within these texts, making information more accessible and analyzable \autocite{Javija2024}. IE systems need to be cost-effective, adaptable, and capable of scaling across domains. In fields like finance, Named-Entity Recognition (NER) is used to extract predefined data types, such as names and dates, from documents, facilitating efficient data management (Gupta2020). Automated learning in IE reduces errors and dependency on manual supervision, making the process more efficient and contextually valuable. The increasing volume of unstructured data, particularly online, emphasizes the importance of effective IE systems \autocite{Javija2024}.

Enkele IE technieken zijn maar niet gelimiteerd tot \autocite{IBM2024}:
\begin{itemize}
  \item Feature selection en Feature extraction
  \begin{itemize}
    \item Feature selection: is een essentiële stap bij het verwerken van gegevens met een groot aantal dimensies. Het gaat om het kiezen van een kleinere set belangrijke kenmerken uit de originele set om de efficiëntie en nauwkeurigheid van het leren te verbeteren. Door overbodige en inconsequente kenmerken te elimineren, wordt de omvang van de gegevensverwerking verkleind, wordt de tijd die nodig is voor het leren verminderd en worden de resultaten gestroomlijnd. Eigenschapsselectie is een proces dat de belangrijkste oorspronkelijke kenmerken behoudt, in tegenstelling tot kenmerkextractie waarbij gegevens worden veranderd in kenmerken die goed zijn in het herkennen van patronen. Eigenschapsselectie is cruciaal voor het verminderen van de dimensionaliteit van gegevens. Technieken voor kenmerkselectie omvatten een reeks benaderingen, zoals supervised, unsupervised en semi-supervised modellen. Deze methoden worden geclassificeerd op basis van hun associatie met leermethoden (filter, wrapper, inbeddingsmodellen) en andere criteria. Eigenschapsselectie is een veelgebruikte techniek in gebieden zoals beeldherkenning en tekst mining. Het verbetert de prestaties van modellen voor machinaal leren door een evenwicht te bereiken tussen hoge nauwkeurigheid en lage rekenvereisten \autocite{CAI201870}.
    \item Feature extraction: is een essentiële stap in machinaal leren, omdat het uitgebreide invoergegevens omzet in een beter hanteerbare en lager-dimensionale kenmerkenset. Deze procedure vereenvoudigt de gegevens door de complexiteit ervan te verminderen, terwijl belangrijke informatie toch behouden blijft.Het is vooral nuttig bij taken zoals categorisatie.Kenmerkextractietechnieken transformeren de initiële kenmerkruimte in een gecondenseerde, alternatieve ruimte door een gereduceerde, representatieve verzameling kenmerken te behouden in plaats van ze weg te gooien.Principale Componenten Analyse (PCA) en Bag of Words zijn vaak gebruikte technieken.Principal Component Analysis (PCA) vermindert bijvoorbeeld de dimensionaliteit van gegevens door de oorspronkelijke variabelen om te zetten in ongecorreleerde componenten.Dit proces verbetert de rekenefficiëntie en verhoogt de nauwkeurigheid van modellen voor machinaal leren \autocite{Mustazzihim}.
    \item Verschil?: FS behoud de orginele features terwijl FE nieuwe maakt.
  \end{itemize}
    \item Named Entity Recognition (NER) is a core job in Natural Language Processing (NLP) that aims to recognise and categorise entities, such as individuals, organisations, and places, within a given text. NER, or Named Entity Recognition, is extensively utilised in a variety of applications, spanning from information retrieval to automated customer care.

    Recent research emphasises that although NER models have attained remarkable performance on typical datasets, frequently exhibiting high F-scores, this measure alone does not offer a thorough comprehension of their efficacy. For instance, cutting-edge NER models typically exhibit F-scores over 90\% on datasets such as OntoNotes. Nevertheless, this solitary metric may obscure variations in performance across various categories of entities, kinds of language, and unfamiliar data \autocite{vajjala2022reallyknowstateart}.
    
\end{itemize}

\paragraph{conclusie}
Information Retrieval (IR) en Information Extraction (IE) zijn twee technologieën die informatie toegankelijk maken via verschillende methodologieën.IR richt zich op het ophalen van relevante documenten, terwijl IE specifieke, gestructureerde informatie extraheert voor nauwkeurige gegevensanalyse.IR is essentieel voor grote datasets en zoekmachines, terwijl IE cruciaal is voor het extraheren van bruikbare inzichten.De kracht van IR ligt in het beheren en ophalen van informatie uit ongestructureerde bronnen, waardoor het onmisbaar is voor grote databases. IE is van vitaal belang voor datamining, kennisbeheer en geautomatiseerde processen.Naarmate het datavolume toeneemt, zal de wisselwerking tussen IR en IE steeds belangrijker worden.Het begrijpen en benutten van beide technologieën zal cruciaal zijn voor het optimaliseren van informatieverwerkingssystemen en om ervoor te zorgen dat gebruikers snel en accuraat de benodigde informatie kunnen verkrijgen.

\begin{table}[h!]
  \centering
  \begin{tabular}{|p{4cm}|p{5cm}|p{5cm}|}
  \hline
  \textbf{Aspect} & \textbf{Information Retrieval} & \textbf{Information Extraction} \\ \hline
  \textbf{Focus} & Document Retrieval & Feature Retrieval \\ \hline
  \textbf{Output} & Return set of relevant documents & Return facts out of documents \\ \hline
  \textbf{Goal} & The goal is to find documents that are relevant to the user’s information need & The goal is to extract pre-specified features from documents or display information. \\ \hline
  \textbf{Nature of Information} & Real information is buried inside documents & Extract information from within the documents \\ \hline
  \textbf{Result Format} & The long listing of documents & Aggregate over the entire set \\ \hline
  \textbf{Application} & Used in many search engines – Google is the best IR system for the web. & Used in database systems to enter extracted features automatically. \\ \hline
  \textbf{Methodology} & Typically uses a bag of words model of the source text. & Typically based on some form of semantic analysis of the source text. \\ \hline
  \textbf{Theoretical Basis} & Mostly use the theory of information, probability, and statistics. & Emerged from research into rule-based systems. \\ \hline
  \end{tabular}
  \caption{Comparison of Information Retrieval and Information Extraction}
  \label{tab:ir_vs_ie}
  \end{table}
  
\subsubsection{NLP}
\paragraph{Summarization}

Een andere kritische NLP techniek is tekstsamenvatting, waarbij een beknopte weergave van originele tekstdocumenten wordt gegenereerd. Dit proces omvat voorbewerkingsstappen zoals tokeniseren, stopwoorden verwijderen en stemmen, gevolgd door het creëren van lexiconlijsten tijdens de verwerkingsfase. Historisch gezien was het samenvatten van tekst gebaseerd op woordfrequentie, maar moderne methoden maken gebruik van geavanceerde text mining technieken om de relevantie en nauwkeurigheid van de resultaten te verbeteren. Kenmerken zoals zinslengte, thematische woorden en vaste zinnen worden gebruikt om belangrijke informatie te extraheren en deze technieken kunnen op meerdere documenten tegelijk worden toegepast\autocite{Talib2016TextMining}.

\paragraph{Part of speech tagging}
Part-of-speech (POS) tagging is een essentiële activiteit in natuurlijke taalverwer-king (NLP) waarbij een grammaticale classificatie, zoals zelfstandig naamwoord,werkwoord of bijvoeglijk naamwoord, wordt toegewezen aan elk woord in een zin.Tagging vergemakkelijkt computationeel begrip van de syntactische organisatievan tekst, een kritisch onderdeel voor veel toepassingen van natuurlijke taalverwer-king (NLP) (Martinez,2012).Ondanks de uitdagingen zoals tweeslachtige woorden bereiken moderne POS tag-gers hoge nauwkeurigheidspercentages (rond 96-97\%) en worden ze veel gebruiktbij het ophalen van informatie, tekstanalyse en andere NLP-taken.(Martinez,2012)
\paragraph{Text categorization}
Tekstclassificatie is een methodische procedure die bestaat uit vier essentiële stap-pen: kenmerken extraheren, de dimensionaliteit verminderen, een classificator se-lecteren en de resultaten evalueren.Eerst wordt tekst omgezet in een numeriek for-maatdoormiddelvankenmerkextractie, zoalswoordfrequentieofWord2Vec.Dimensionaliteitsreductiewordt gebruikt om de gegevens te vereenvoudigen en cruciale informatie te be-houden.Dit wordt bereikt door technieken zoals principale componentenanalyse(PCA) of lineaire discriminantanalyse (LDA) toe te passen.De selectie van een clas-sifier is essentieel, omdat deep learning-methoden vaak conventionele machinelearning-algoritmen overtreffen in termen van nauwkeurigheid.Uiteindelijk wordtde doeltreffendheid van het model beoordeeld door de prestaties te meten met be-hulp van metrieken zoals de Matthews correlatiecoëfficiënt (MCC), oppervlakte on-der de ROC-curve (AUC) en nauwkeurigheid.Van deze maatstaven wordt nauwkeu-righeid beschouwd als de meest directe en eenvoudige manier om de prestatiesvan het model te evalueren. Gupta et al. (2020) ontdekten dat supportvectorma-chines (SVM) beter presteerden dan andere benaderingen zoals Naive Bayes (NB),k-nearest neighbour (KNN), beslisbomen en regressie in termen van nauwkeurig-heid, recall en F1-maatstaven.
\paragraph{Sentiment analysis}
Natural Language Processing (NLP) omvat verschillende technieken om onnauwkeurig en dubbelzinnig taalgebruik om te zetten in nauwkeurige en ondubbelzinnige berichten, met toepassingen in sectoren als financiën, e-commerce en sociale media. Een belangrijke techniek binnen NLP is Sentimentanalyse (SA), ook wel bekend als opinion mining, waarbij onderliggende meningen uit tekstgegevens worden gehaald. SA is vooral nuttig voor taken als emotieherkenning en polariteitsdetectie, met toepassingen variërend van voorspelling van de aandelenmarkt tot analyse van feedback van klanten \autocite{Gupta2020}.

SA kan worden benaderd met lexicongebaseerde methoden, die vertrouwen op tools zoals SentiWordNet voor woord-naar-sentiment mapping, of machine learning (ML) technieken die tekst classificeren met behulp van algoritmes zoals Naïve Bayes (NB) en support vector machines (SVM's). Hoewel ML-benaderingen geen kostbare woordenboeken vereisen, vereisen ze wel domeinspecifieke datasets, wat een beperking kan zijn. Bovendien zijn deep learning-methoden onlangs gecombineerd met traditionele ML-technieken om de nauwkeurigheid en betrouwbaarheid van SA te verbeteren, met name in financiële voorspellingen \autocite{Gupta2020}.

\subsubsection{Data mining}
















% ----------------------------------------------------------------------------
\section{Technieken en Tools}
\subsection{Spacy vs NLTK}
Tabel \ref{tab:comparison} geeft een vergelijkende analyse van SpaCy en NLTK op basis van belangrijke functies die relevant zijn voor tekstsamenvatting.

\begin{table}[h!]
\centering
\caption{Vergelijkende Analyse van SpaCy en NLTK}
\label{tab:comparison}
\begin{tabular}{@{}|l|l|l|@{}}
\toprule
\textbf{Functie} & \textbf{NLTK} & \textbf{SpaCy} \\ \midrule
\textbf{Precisie} & 0.51 & 0.72 \\ \midrule
\textbf{Recall} & 0.65 & 0.65 \\ \midrule
\textbf{F-Score} & 0.58 & 0.69 \\ \midrule
\textbf{Tokenisatiesnelheid} & 4 ms & 0.2 ms \\ \midrule
\textbf{Taggingsnelheid} & 443 ms & 1 ms \\ \midrule
\textbf{Ondersteuning voor Classificatie} & Ja & Ja \\ \midrule
\textbf{Onderwerpmodellering} & Nee & Ja \\ \midrule
\textbf{Vectorisatie} & Nee & Ja \\ \midrule
\textbf{Parsing} & Ja & Ja \\ \midrule
\textbf{TF-IDF Implementatie} & Nee & Ja \\ \midrule
\textbf{Programmeerparadigma} & Procedureel & Objectgeoriënteerd \\ \midrule
\textbf{Gebruiksvriendelijkheid} & Vereist meer aanpassing en tijd & Meer geautomatiseerd en gebruiksvriendelijk \\ \midrule
\textbf{Ondersteunde Taalmodellen} & Basis tokenisatie en parsing & Geavanceerde modellen met voorgetrainde vectors \\ \midrule
\textbf{Grootte en Afhankelijkheden van de Bibliotheek} & Lichtgewicht, minimale afhankelijkheden & Zwaarder, meer afhankelijkheden door geavanceerde functies \\ \bottomrule
\end{tabular}
\end{table}

\textbf{Prestatiewaarden (Precisie, Recall, F-Score):} SpaCy toont hogere precisie (0.72 vs. 0.51) en F-Score (0.69 vs. 0.58) in vergelijking met NLTK, wat wijst op superieure nauwkeurigheid bij het genereren van samenvattingen.

\textbf{Snelheid van Tokenisatie en Tagging:} SpaCy is aanzienlijk sneller dan NLTK in zowel tokeniseren als taggen. SpaCy kan bijvoorbeeld tekst tokenen in 0,2 milliseconden vergeleken met de 4 milliseconden van NLTK, waardoor het geschikter is voor toepassingen die real-time verwerking vereisen.
\textbf{Ondersteuning voor Geavanceerde NLP-functies:} SpaCy ondersteunt geavanceerde functies zoals topic modellering, vectorisatie en TF-IDF (Term Frequency-Inverse Document Frequency), die niet standaard beschikbaar zijn in NLTK. Dit maakt SpaCy een meer uitgebreide tool voor taken die diep semantisch begrip en machine learning integratie vereisen.


\textbf{Gebruiksvriendelijkheid:} SpaCy is gebruiksvriendelijker met ingebouwde mogelijkheden, wat de behoefte aan maatwerkprogrammering vermindert, in tegenstelling tot NLTK, dat meer tijd en inspanning vergt.

\paragraph{subsubsection}{Conclusie}
Samenvattend, SpaCy is een krachtigere en efficiëntere tool voor tekstsamenvatting vanwege zijn hogere precisie, snelheid en ondersteuning voor geavanceerde NLP-functies. NLTK, hoewel veelzijdig, is beter geschikt voor eenvoudigere taken of projecten die meer aanpassing vereisen. De keuze tussen deze tools hangt af van de specifieke eisen van het project, waaronder de complexiteit van de taak, de benodigde functies en de beschikbare middelen.



\subsection{Database Management Systemen (DBMS)}
In deze sectie gaan wij bekijken welke databank gebruikt zal worden na het structuren en standardiseren van de 13f meldingen. Hier zal besproken worden of er SQL of nosql gebruikt zal worden vervolgens zal er een specifieke databank gekozen worden
\subsubection{SQL vs NOSQL}
Bij het kiezen tussen SQL- en NoSQL-databases is het belangrijk om de onderliggende architectuur en toepassingsmogelijkheden te begrijpen. SQL-databases zijn ontworpen voor het organiseren van gestructureerde data, waardoor ze ideaal zijn voor online transaction processing (OLTP). Ze presteren uitstekend in situaties waarin complexe queries, consistentie en relationeel databeheer vereist zijn. NoSQL-databases daarentegen ondersteunen horizontale schaalbaarheid en zijn geoptimaliseerd voor het verwerken van grote hoeveelheden ongestructureerde data, wat hen geschikt maakt voor big data-analyse. De keuze tussen beide hangt grotendeels af van de specifieke behoeften van de organisatie, zoals de focus op datastructuur of schaalbaarheid \autocite{khan2023performance}.

In dit onderzoek is gekozen voor een SQL-database. Deze keuze is gebaseerd op de noodzaak om gestructureerde data uit de 13F-meldingen te beheren, waarbij consistente gegevensintegriteit en de mogelijkheid om complexe queries uit te voeren cruciaal zijn. SQL-databases bieden de benodigde functionaliteiten voor het beheer van relationele gegevens en het uitvoeren van geavanceerde analyses, wat essentieel is voor het succes van dit project \autocite{khan2023performance}.

\paragraph{SQL databank}
Op basis van de gedetailleerde analyse in het GeeksforGeeks-artikel werd PostgreSQL gekozen voor ons proefschrift vanwege de geavanceerde functies, robuuste gegevensintegriteit en uitbreidbare architectuur. In tegenstelling tot andere SQL databases, blinkt PostgreSQL uit in het verwerken van complexe datamanipulatie, het bieden van sterke ACID compliance en het ondersteunen van aangepaste datatypes en functies. Dit maakt PostgreSQL bijzonder geschikt voor bedrijfstoepassingen en datawarehousing waar schaalbaarheid en geavanceerd databeheer cruciaal zijn. Hoewel PostgreSQL een steilere leercurve heeft dan sommige alternatieven, maken de uitgebreide functieset en betrouwbaarheid het een optimale keuze om aan de complexe eisen van ons project te voldoen.

\subsubsection{Review}
Dus voor dit onderzoek hebben we voor de data mining gekozen voor Spacy en voor de DB PostgreSQL








% ----------------------------------------------------------------------------


\section{Uitdagingen en beperkingen}
\subsection{Complexiteit van financiële Tekst}
\subsection{Gegevenskwaliteit en Validatie}
\subsection{Databaseprestaties}
Uitdaging: Hoewel PostgreSQL goed presteert bij grote hoeveelheden gestructureerde data, kan het moeilijk zijn om de prestaties te optimaliseren naarmate de hoeveelheid data en het aantal gelijktijdige gebruikers toeneemt.

Beperking: Bij zeer grote datasets of een hoge mate van gelijktijdige toegang kunnen er prestatieproblemen optreden. Het kan nodig zijn om uitgebreide optimalisaties en schaalstrategieën te implementeren, zoals partitionering of het gebruik van read replicas.
% ----------------------------------------------------------------------------
\section{Leemtes in huidig onderzoek}
\subsection{Onbehandelde kwesties}
Training van Eigen Large Language Models (LLMs)
Het trainen van een eigen LLM voor financiële toepassingen vereist veel tijd en middelen. Het model moet worden getraind op uitgebreide financiële datasets voor nauwkeurige resultaten. Dit kan uw infrastructuur belasten en vereist expertise in machine learning en datawetenschap, met mogelijke problemen op het gebied van data-integriteit en privacy. Overweeg het gebruik van bestaande financiële NLP-modellen die al getraind zijn en efficiënt kunnen worden aangepast aan uw behoeften.

Beveiliging en Privacy
Het beschermen van gevoelige financiële gegevens tegen ongeautoriseerde toegang en datalekken is complex en vereist naleving van privacywetgeving. Onvoldoende beveiliging kan leiden tot datalekken, verlies van vertrouwen en juridische problemen. Implementeer encryptie, toegangscontrole en regelmatige beveiligingsaudits om gegevens te beschermen. Zorg ervoor dat uw systemen voldoen aan relevante regelgeving en best practices voor gegevensbeveiliging.

Schaalbaarheid en Prestaties
Groeiende hoeveelheden gegevens kunnen leiden tot prestatieproblemen bij opslag en analyse, wat complexe oplossingen vereist voor snelle toegang. Slechte prestaties kunnen vertragingen veroorzaken in rapportage en analyse, wat de besluitvorming en efficiëntie beïnvloedt. Gebruik schaalbare databases en technieken zoals gegevenspartitionering en caching. Monitor en optimaliseer regelmatig de prestaties om problemen te voorkomen.

\subsection{Verbeteringsmogelijkehden}
% ----------------------------------------------------------------------------

\section{conclusie}
\subsection{Samenvatting van Bevindingen}
\subsection{Implicaties van het onderzoek}